{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "41dbe8df44a590ae",
      "metadata": {
        "id": "41dbe8df44a590ae"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "from torch.utils.data import Dataset, DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97f42d4836e515df",
      "metadata": {
        "id": "97f42d4836e515df"
      },
      "source": [
        "### Dataset scraping and preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "initial_id",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "initial_id",
        "outputId": "60e38dc0-3d08-4459-e1e8-505e4ffdd729"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Scraping and preparing data\n"
          ]
        }
      ],
      "source": [
        "print(\"Scraping and preparing data\")\n",
        "url = \"https://www.gutenberg.org/cache/epub/84/pg84-images.html\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, 'html.parser')\n",
        "raw_text = soup.get_text()\n",
        "tokens = re.findall(r'\\b\\w+\\b', raw_text.lower())\n",
        "\n",
        "# Build Vocabulary\n",
        "vocab = sorted(list(set(tokens)))\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Define window_size variable\n",
        "window_size = 100\n",
        "seq_length = window_size - 1 # 99 tokens for input\n",
        "\n",
        "inputs = []\n",
        "targets = []\n",
        "\n",
        "# Format into lists where inner lists contain 99 numbers\n",
        "for i in range(len(tokens) - window_size + 1):\n",
        "    seq_in = tokens[i : i + seq_length]\n",
        "    seq_out = tokens[i + seq_length]\n",
        "\n",
        "    # Input sequence and target are consecutive\n",
        "    inputs.append([word_to_idx[w] for w in seq_in])\n",
        "    targets.append(word_to_idx[seq_out])\n",
        "\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, x_data, y_data):\n",
        "        self.x = torch.tensor(x_data, dtype=torch.long)\n",
        "        self.y = torch.tensor(y_data, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.x)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.x[idx], self.y[idx]\n",
        "\n",
        "dataset = TextDataset(inputs, targets)\n",
        "dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ad25af60a9b40e93",
      "metadata": {
        "id": "ad25af60a9b40e93"
      },
      "source": [
        "### RNN model architecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3a134f1b8311676d",
      "metadata": {
        "id": "3a134f1b8311676d"
      },
      "outputs": [],
      "source": [
        "class TextGenerationRNN(nn.Module):\n",
        "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
        "        super(TextGenerationRNN, self).__init__()\n",
        "        # Embedding layer\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
        "        # one RNN layer\n",
        "        self.rnn = nn.RNN(embed_size, hidden_size, batch_first=True)\n",
        "        # Fully connected layer for output\n",
        "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_length)\n",
        "        embedded = self.embedding(x)\n",
        "        # out shape: (batch_size, seq_length, hidden_size)\n",
        "        out, hidden = self.rnn(embedded)\n",
        "        # Take the output of the final token in the sequence to predict the next word\n",
        "        last_out = out[:, -1, :]\n",
        "        logits = self.fc(last_out)\n",
        "        return logits\n",
        "\n",
        "# Hyperparameters\n",
        "embed_size = 64\n",
        "hidden_size = 128\n",
        "learning_rate = 0.005\n",
        "epochs = 15\n",
        "\n",
        "model = TextGenerationRNN(vocab_size, embed_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d67b2f4787910206",
      "metadata": {
        "id": "d67b2f4787910206"
      },
      "source": [
        "### Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6fe2c2e9b2d02638",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6fe2c2e9b2d02638",
        "outputId": "c1204386-1da2-466a-9405-5f9c318b6760"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting training...\n",
            "Epoch 1/15 | Loss: 6.6138\n",
            "Epoch 3/15 | Loss: 5.7538\n",
            "Epoch 6/15 | Loss: 6.2714\n",
            "Epoch 9/15 | Loss: 5.0601\n",
            "Epoch 12/15 | Loss: 4.8546\n",
            "Epoch 15/15 | Loss: 5.5523\n"
          ]
        }
      ],
      "source": [
        "print(\"Starting training...\")\n",
        "model.train()\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    if (epoch + 1) % 3 == 0 or epoch == 0:\n",
        "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {epoch_loss/len(dataloader):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f94108ad",
      "metadata": {
        "id": "f94108ad"
      },
      "source": [
        "### Continue Model Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "r3RZrf-lJkW7",
      "metadata": {
        "id": "r3RZrf-lJkW7"
      },
      "outputs": [],
      "source": [
        "epochs = 24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5bccca52",
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "5bccca52",
        "outputId": "86fca712-35f7-48ae-eedd-7682f14582cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Continuing training from epoch 15...\n",
            "Epoch 15/24 | Loss: 5.8290\n",
            "Epoch 18/24 | Loss: 5.5928\n",
            "Epoch 21/24 | Loss: 4.8260\n",
            "Epoch 24/24 | Loss: 4.5780\n",
            "Training complete.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Continuing training from epoch {epoch + 1}...\")\n",
        "model.train()\n",
        "# Continue training from the last completed epoch up to the total number of epochs\n",
        "for current_epoch in range(epoch, epochs):\n",
        "    epoch_loss = 0\n",
        "    for batch_inputs, batch_targets in dataloader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(batch_inputs)\n",
        "        loss = criterion(outputs, batch_targets)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    if (current_epoch + 1) % 3 == 0 or current_epoch == 0:\n",
        "        print(f\"Epoch {current_epoch+1}/{epochs} | Loss: {epoch_loss/len(dataloader):.4f}\")\n",
        "\n",
        "print(\"Training complete.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "727ea6eb9ba04fc1",
      "metadata": {
        "id": "727ea6eb9ba04fc1"
      },
      "source": [
        "### Text Generation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "c694ab3ab222cc36",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c694ab3ab222cc36",
        "outputId": "8877d92c-8e2e-4b2c-8a80-578fda662f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Generating text...\n",
            "\n",
            "--- Final Generated Text ---\n",
            "the monster looked at me and extinguish which i had arrived at the alteration except the growth of chamounix the cabin where i had arrived at the project gutenberg concept and the alps of the american hemisphere and tingling despairing and i had no right to the treacherous turk i cannot describe the uses of the project gutenberg ebook frankenstein the resources of the project gutenberg concept and ardent works in accordance and the range of the higher of chamounix the silence of the project gutenberg concept and the alps of the american hemisphere and tingling despairing and i shunned the inclemency of the project gutenberg\n"
          ]
        }
      ],
      "source": [
        "def generate_text(model, seed_words, num_words_to_generate=100):\n",
        "    model.eval()\n",
        "    words = seed_words.lower().split()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # generate a text of at least 100 words\n",
        "        for _ in range(num_words_to_generate):\n",
        "            # Take the most recent context window (up to 99 tokens)\n",
        "            context_words = words[-seq_length:]\n",
        "\n",
        "            # Convert context to indices (defaulting to 0 for unknown words in seed)\n",
        "            context_indices = [word_to_idx.get(w, 0) for w in context_words]\n",
        "            x_tensor = torch.tensor([context_indices], dtype=torch.long)\n",
        "\n",
        "            # Predict the next word\n",
        "            prediction = model(x_tensor)\n",
        "            predicted_idx = torch.argmax(prediction, dim=-1).item()\n",
        "            predicted_word = idx_to_word[predicted_idx]\n",
        "            words.append(predicted_word)\n",
        "\n",
        "    return ' '.join(words)\n",
        "\n",
        "print(\"\\nGenerating text...\")\n",
        "seed_sentence = \"the monster looked at me and\"\n",
        "generated_output = generate_text(model, seed_sentence, num_words_to_generate=100)\n",
        "\n",
        "print(\"\\n--- Final Generated Text ---\")\n",
        "print(generated_output)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}