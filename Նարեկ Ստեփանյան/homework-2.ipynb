{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2026-02-23T06:16:21.010393897Z",
     "start_time": "2026-02-23T06:14:55.492094683Z"
    }
   },
   "source": [
    "import re\n",
    "import math\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from collections import Counter\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "url = \"https://www.gutenberg.org/cache/epub/84/pg84-images.html\"\n",
    "html = requests.get(url, timeout=30).text\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "raw_text = soup.get_text(separator=\" \")\n",
    "\n",
    "raw_text = re.sub(r\"\\s+\", \" \", raw_text).strip()\n",
    "\n",
    "# 2) Tokenize\n",
    "\n",
    "tokens = re.findall(r\"[a-zA-Z']+|[.!?;,:\\-]\", raw_text.lower())\n",
    "\n",
    "PAD = \"<PAD>\"\n",
    "UNK = \"<UNK>\"\n",
    "\n",
    "min_freq = 5\n",
    "cnt = Counter(tokens)\n",
    "\n",
    "vocab = [PAD, UNK] + [w for w, c in cnt.items() if c >= min_freq]\n",
    "word2id = {w: i for i, w in enumerate(vocab)}\n",
    "id2word = {i: w for w, i in word2id.items()}\n",
    "\n",
    "ids = [word2id.get(w, word2id[UNK]) for w in tokens]\n",
    "\n",
    "window_size = 100\n",
    "seq_len = window_size - 1\n",
    "\n",
    "X = []\n",
    "y = []\n",
    "for i in range(0, len(ids) - window_size):\n",
    "    X.append(ids[i:i+seq_len])\n",
    "    y.append(ids[i+seq_len])\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.long)\n",
    "y = torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "class SeqDS(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X, self.y = X, y\n",
    "    def __len__(self):\n",
    "        return self.y.size(0)\n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]\n",
    "\n",
    "batch_size = 64\n",
    "loader = DataLoader(SeqDS(X, y), batch_size=batch_size, shuffle=True)\n",
    "\n",
    "class RNNTextGen(nn.Module):\n",
    "    def __init__(self, vocab_size, emb_dim=128, hidden_dim=256):\n",
    "        super().__init__()\n",
    "        self.emb = nn.Embedding(vocab_size, emb_dim, padding_idx=word2id[PAD])\n",
    "        self.rnn = nn.RNN(emb_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        e = self.emb(x)\n",
    "        out, _ = self.rnn(e)\n",
    "        last = out[:, -1, :]\n",
    "        logits = self.fc(last)\n",
    "        return logits\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = RNNTextGen(vocab_size=len(vocab)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.003)\n",
    "\n",
    "#Train\n",
    "epochs = 15\n",
    "for ep in range(1, epochs + 1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for xb, yb in loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    avg_loss = total_loss / len(loader.dataset)\n",
    "    ppl = math.exp(avg_loss) if avg_loss < 20 else float(\"inf\")\n",
    "    print(f\"Epoch {ep:02d} | loss={avg_loss:.4f} | ppl~{ppl:.1f}\")\n",
    "\n",
    "# 7) Text generation\n",
    "\n",
    "model.eval()\n",
    "\n",
    "seed_text = \"the creature was\"\n",
    "seed_tokens = re.findall(r\"[a-zA-Z']+|[.!?;,:\\-]\", seed_text.lower())\n",
    "seed_ids = [word2id.get(w, word2id[UNK]) for w in seed_tokens]\n",
    "\n",
    "generated = seed_tokens[:]\n",
    "temperature = 0.9\n",
    "gen_words = 120\n",
    "\n",
    "context = seed_ids[:]\n",
    "if len(context) < seq_len:\n",
    "    context = [word2id[PAD]] * (seq_len - len(context)) + context\n",
    "else:\n",
    "    context = context[-seq_len:]\n",
    "\n",
    "for _ in range(gen_words):\n",
    "    x = torch.tensor([context], dtype=torch.long).to(device)\n",
    "    with torch.no_grad():\n",
    "        logits = model(x).squeeze(0)\n",
    "        logits = logits / temperature\n",
    "        probs = torch.softmax(logits, dim=0)\n",
    "\n",
    "        next_id = torch.multinomial(probs, num_samples=1).item()\n",
    "\n",
    "    next_tok = id2word.get(next_id, UNK)\n",
    "    generated.append(next_tok)\n",
    "\n",
    "    context = context[1:] + [next_id]\n",
    "\n",
    "out = []\n",
    "for t in generated:\n",
    "    if t in [\".\", \"!\", \"?\", \",\", \";\", \":\"]:\n",
    "        if out:\n",
    "            out[-1] = out[-1] + t\n",
    "        else:\n",
    "            out.append(t)\n",
    "    elif t == \"-\":\n",
    "        out.append(\"-\")\n",
    "    else:\n",
    "        out.append(t)\n",
    "\n",
    "print(\"\\nGenerated text:\\n\")\n",
    "print(\" \".join(out))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01 | loss=5.0348 | ppl~153.7\n",
      "Epoch 02 | loss=4.6415 | ppl~103.7\n",
      "Epoch 03 | loss=4.4769 | ppl~88.0\n",
      "Epoch 04 | loss=4.4151 | ppl~82.7\n",
      "Epoch 05 | loss=4.5079 | ppl~90.7\n",
      "Epoch 06 | loss=4.7056 | ppl~110.6\n",
      "Epoch 07 | loss=4.7457 | ppl~115.1\n",
      "Epoch 08 | loss=4.5494 | ppl~94.6\n",
      "Epoch 09 | loss=4.5064 | ppl~90.6\n",
      "Epoch 10 | loss=4.6405 | ppl~103.6\n",
      "Epoch 11 | loss=4.6263 | ppl~102.1\n",
      "Epoch 12 | loss=4.9803 | ppl~145.5\n",
      "Epoch 13 | loss=4.6360 | ppl~103.1\n",
      "Epoch 14 | loss=4.3949 | ppl~81.0\n",
      "Epoch 15 | loss=4.2356 | ppl~69.1\n",
      "\n",
      "Generated text:\n",
      "\n",
      "the creature was dry me, or rather i ought were somewhat, <UNK> were extreme him. i will henry a plain that she was on earth and had been the favourite - <UNK> of the streets little. this frequently were <UNK> the clouds i touched on his feelings on in one, but revenge, dear victor every hate him - fiend of consideration, i ran were, i looked upon their dying continually addressed to hold his body, and day, but there consented by the creation of the habitation, but with an conclusion, and i demand, and every regard of the house of support by different and afterwards sat of our story i\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
